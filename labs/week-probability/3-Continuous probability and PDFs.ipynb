{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "original version: Jessica Hamrick and Tom Griffiths\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import  pyplot as plt\n",
    "from ipywidgets import interact\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous Probability and Probability Densities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have talked about probability spaces in which we have a finite (or countably infinite) number of sample points. In real life, however, many of quantities that we wish to model are _real-valued_ -- examples include the time between earthquakes along the San Andreas fault, the location of a robot within an environment, and the angle of deflection of a photon on a sensor.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuous Random Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin, consider the discrete random variable $X$ which measures the probability that a robot is in a particular state. Let us further assume that the robot can only be in a single state at a time, and that there are only 6 total states the robot could possibly take. We can summarize the probability mass function for $X$ in the following table:\n",
    "\n",
    "| state | P(X=state) |\n",
    "|:-------:|:------------:|\n",
    "|    1    |      1/6     |\n",
    "|    2    |      1/12     |\n",
    "|    3    |      1/6     |\n",
    "|    4    |      1/3     |\n",
    "|    5    |      1/12     |\n",
    "|    6    |      1/6     |\n",
    "\n",
    "We can also represent these probabilities visually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.arange(1,7)\n",
    "probs = [1./6, 1./12, 1./6, 1./3, 1./12, 1./6]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(labels, probs, align='center');\n",
    "ax.set_xticks(labels)\n",
    "ax.set_xlabel('State');\n",
    "ax.set_ylabel('P(X = state)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's imagine that we decide to be a bit more precise in the way we\n",
    "characterize the robot's states. Imagine that we make an extra distinction modes\n",
    "$a$ and $b$ for each of the 6 states we plotted earlier (i.e., we now distinguish\n",
    "between state $1a$ and state $1b$ where $P(X=1a) + P(X=1b)$ equals $P(X=1)$ from the first table). This doubles the size of our sample space:\n",
    "\n",
    "| state | P(X=state) |\n",
    "|:-----:|:----------:|\n",
    "|   1a  |     1/6    |\n",
    "|   1b  |      0     |\n",
    "|   2a  |    1/24    |\n",
    "|   2b  |    1/24    |\n",
    "|   3a  |    1/24    |\n",
    "|   3b  |     1/8    |\n",
    "|   4a  |     1/9    |\n",
    "|   4b  |     2/9    |\n",
    "|   5a  |    1/12    |\n",
    "|   5b  |      0     |\n",
    "|   6a  |    1/18    |\n",
    "|   6b  |     1/9    |\n",
    "\n",
    "Visually, this looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locs = np.arange(1,13)\n",
    "labels = ['1a', '1b', '2a', '2b', '3a', '3b', '4a', '4b', '5a', '5b', '6a', '6b'] \n",
    "probs = [1./6, 0., 1./24, 1./24, 1./24, 1./8, 1./9, 2./9, 1./12, 0., 1./18, 1./9]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(locs, probs, align='center');\n",
    "ax.set_xticks(locs)\n",
    "ax.set_xticklabels(labels);\n",
    "ax.set_xlabel('State');\n",
    "ax.set_ylabel('P(X = state)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In comparing Table/Plot 1 to Table/Plot 2, notice that the probability of the robot being in any particular state has either remained the same or decreased. \n",
    "\n",
    "For a more extreme example, imagine a uniform distribution over a discrete sample space. When there is only a single point in the sample space, the probability of that point must necessarily be 1. However, when we add a new point to the space, we must divide the probability mass between the original sample point and the new point; if we divide it equally, each sample point will now have a probability of $0.5$. As we continue adding sample points to the space, we can see that the probability of any individual sample point gets smaller and smaller. You can visualize this with the following slider:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact\n",
    "def plot_pmf(num_sample_points=(1, 50, 1)):\n",
    "    probs = np.ones(num_sample_points) * (1./num_sample_points)\n",
    "    locs = np.arange(num_sample_points) + 1\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    ax.bar(locs, probs, align='center');\n",
    "    ax.set_xticks(locs);\n",
    "    ax.set_ylim([0, 1.]);\n",
    "    ax.set_xlabel('x');\n",
    "    ax.set_ylabel('P(X = x)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an important insight:\n",
    "\n",
    "> As we increase the number of points in our sample space, the probability associated with each individual point shrinks\n",
    "\n",
    "We can extrapolate this notion to the case where there are an (uncountably) infinite number of states in our sample space. In this case, the probability of being in any single state shrinks to 0. State spaces of this type are called continuous spaces. The random variables that operate on these state spaces are called (surprise!) continuous random variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuous Random Variables\n",
    "\n",
    "In contrast to discrete random variables, a continuous random variable can take any of an (uncountably) infinite number of values. This infinite sample space complicates things somewhat. \n",
    "1. Whereas we could express their probability distributions for discrete random variables in the form of a table, we can no longer do so for continuous random variables since this would entail creating a table with an uncountably infinite number of values!\n",
    "2. The probability that a continuous r.v. takes any _specific_ value is 0. Instead, we have to talk about the probability that a continuous random variable will fall within a _range_ of values. To this end, we call the probability functions for continuous random variables probability _density_ functions.\n",
    "\n",
    "\n",
    "### Probability Density Functions\n",
    "Unlike the probability mass functions for discrete random variables, the probability distributions for continuous random variables will be _smooth_, since they are defined over a continuous sample space. One example of a continuous probability function is the univariate **Gaussian (normal) distribution**:\n",
    "\n",
    "$$f(x) = \\frac{1}{\\sqrt{2 \\pi} \\sigma} \\exp \\left\\{ -\\frac{(x-\\mu)^2}{2\\sigma^2}\\right\\} $$\n",
    "\n",
    "Below, we plot the univariate normal distribuion with mean $\\mu = 0$ and standard deviation $\\sigma =1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian(x, mu, sigma): \n",
    "    \"\"\"\n",
    "    Evaluate and return the Gaussian function with mean mu\n",
    "    and standard deviation sigma at the values in x.\n",
    "    \"\"\"\n",
    "    return np.exp(-( (x - mu) ** 2) / (2 * (sigma ** 2))) / (np.sqrt(2 * np.pi) * sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a normal distribution with mean 0 and standard deviation 1\n",
    "x_vals = np.linspace(-3, 3, 120)\n",
    "plt.plot(x_vals, gaussian(x_vals, 0, 1));\n",
    "plt.xlabel('x');\n",
    "plt.ylabel('P(x)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine a new continuous random variable $X$ whose values follow a normal distribution with mean 0 and standard deviation 1. We would like to calculate the probability that $X$ falls between the values 0 and 1. That is, we wish to calculate $P(0 \\leq x \\leq 1)$. Graphically, this corresponds to the shaded region in the plot below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the area under the curve representing P(0 <= x <= 1)\n",
    "x_vals = np.linspace(-3, 3, 120)\n",
    "plt.plot(x_vals, gaussian(x_vals, 0, 1));\n",
    "plt.xlabel('x');\n",
    "plt.ylabel('P(x)');\n",
    "section = np.arange(0, 1, 1./20)\n",
    "plt.fill_between(section, gaussian(section, 0, 1));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to compute this value, we must integrate our Gaussian equation over the interval $[0, 1]$. That is,\n",
    "\n",
    "$$P(0 \\leq x \\leq 1) = \\int_0^1 \\frac{1}{\\sqrt{2 \\pi} \\sigma} \\exp \\left\\{ -\\frac{(x-\\mu)^2}{2\\sigma^2}\\right\\}\\ dx$$\n",
    "\n",
    "where $\\sigma = 1$ and $\\mu = 0$. This is approximately equal to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computes P(0 < x <= 1) for a standard normal r.v. X\n",
    "from scipy.stats import norm\n",
    "area = norm.cdf(1) - norm.cdf(0)\n",
    "print('P(0 < x <= 1): {}'.format(area))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we use the _cumulative distribution function_ (CDF) for the the standard normal distribution to approximate this integral. For the purposes of this class, however, you will not be asked to evaluate any of these functions yourself (lucky you!). For those interested, have a look at the [error function](https://en.wikipedia.org/wiki/Error_function), which is quite helpful for integrating the Gaussian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
